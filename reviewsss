***FISH***

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

data = pd.read_csv('../input/fish-market/Fish.csv')
df = data.copy()
df.sample(10)

sns.barplot(x=sp.index, y=sp['Species']);
plt.xlabel('Species')
plt.ylabel('Counts of Species')
plt.show()

g = sns.pairplot(df, kind='scatter', hue='Species');

# Dependant (Target) Variable:
y = df1['Weight']
# Independant Variables:
X = df1.iloc[:,2:7]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

reg = LinearRegression()
reg.fit(X_train,y_train)














***diabetics***
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')

#Loading the dataset
diabetes_data = pd.read_csv('../input/diabetes.csv')

#Print the first 5 rows of the dataframe.
diabetes_data.head()

diabetes_data_copy = diabetes_data.copy(deep = True)
diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)

diabetes_data_copy['Glucose'].fillna(diabetes_data_copy['Glucose'].mean(), inplace = True)
diabetes_data_copy['BloodPressure'].fillna(diabetes_data_copy['BloodPressure'].mean(), inplace = True)
diabetes_data_copy['SkinThickness'].fillna(diabetes_data_copy['SkinThickness'].median(), inplace = True)
diabetes_data_copy['Insulin'].fillna(diabetes_data_copy['Insulin'].median(), inplace = True)
diabetes_data_copy['BMI'].fillna(diabetes_data_copy['BMI'].median(), inplace = True)

p = diabetes_data.hist(figsize = (20,20))

p=sns.pairplot(diabetes_data_copy, hue = 'Outcome')

plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X =  pd.DataFrame(sc_X.fit_transform(diabetes_data_copy.drop(["Outcome"],axis = 1),),
        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)

from sklearn.neighbors import KNeighborsClassifier


test_scores = []
train_scores = []

for i in range(1,15):

    knn = KNeighborsClassifier(i)
    knn.fit(X_train,y_train)
    
    train_scores.append(knn.score(X_train,y_train))
    test_scores.append(knn.score(X_test,y_test))



***GLASS***

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn import tree
from sklearn.preprocessing import StandardScaler

df=pd.read_csv('../input/glass/glass.csv')
sns.countplot(x="Type", data=df, palette='bright')
plt.show()

cor = df.corr()
plt.figure(figsize=(9,6))
sns.heatmap(data = cor, annot = True, cmap = 'PiYG')
plt.show()

columns=list(df.columns)
columns.remove('Type')

X=df.drop('Type', axis=1)
Y=df['Type']

X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.20, random_state=42)




dt_gini=DecisionTreeClassifier(criterion='gini')
dt_gini.fit(X_train, Y_train)

*** startups ***

Linear regression

*** computer ***

import numpy as np # linear algebra
import pandas as pd

train_data=pd.read_csv("/kaggle/input/mobile-price-range-prediction-is2020/train_data.csv")
test_data=pd.read_csv("/kaggle/input/mobile-price-range-prediction-is2020/test_data.csv")

x=train_data.drop(columns=['price_range','id'])
x

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 50)
print(x_train.shape)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=100,multi_class='multinomial', random_state=0,max_iter=1000)
lr = lr.fit(x_train, y_train)

pred=lr.predict(x_test)
print(lr.score(x_train, y_train))
print(lr.score(x_test, y_test))

from sklearn.neighbors import KNeighborsClassifier  
knn = KNeighborsClassifier(n_neighbors=3)  
knn.fit(x_train, y_train)

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb = nb.fit(x_train, y_train)
In [16]:
print(nb.score(x_train, y_train))
print(nb.score(x_test, y_test))




from sklearn.svm import SVC
svm = SVC(C=1.0, kernel='linear', random_state=0)
svm = svm.fit(x_train, y_train)
In [23]:
print(svm.score(x_train, y_train))
print(svm.score(x_test, y_test))
final_pred1=svm.predict(test_data)
final_pred1

***tours and travels***

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score

df_travel=pd.read_csv('/kaggle/input/travel-insurance/travel insurance.csv')
df_travel.head()

df_travel=df_travel.drop(['Gender'], axis=1)
df_travel.head()

fig = plt.figure(figsize = (10, 5))
plt.hist(df_travel['Age'])
plt.xlabel("Age")
plt.ylabel("Number of people")
plt.title("Distribution of Age")
plt.show()

sns.pairplot(df_travel, hue="Claim")

fig, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(df_travel.corr(), annot=True, ax=ax)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
preds = model.predict(X_test)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train.values.ravel())
preds = model.predict(X_test)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import tensorflow as tf 
import pathlib
from sklearn.metrics import confusion_matrix ,accuracy_score

data = pd.read_csv(data_apth)

data['Potability'] = data['Potability'].astype('category')

data.fillna(value=data.median(), inplace=True)


*** waterQuality ***

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('../input/water-potability/water_potability.csv')

fig,ax = plt.subplots(figsize = (10,7))
sns.heatmap(df.corr(),annot = True)

train = df.sample(frac = 0.7,random_state = 25)
test  = df.sample(frac = 0.3,random_state = 26)

sns.pairplot(train[['ph','Sulfate','Trihalomethanes']])

for feat in train.columns:
    train[feat] = train[feat].fillna(train[feat].mean())

logistic regression
random forest classifier

model = RandomForestClassifier()
model.fit(X_train_smote,y_train_smote)
y_pred = model.predict(X_test)


***Placement***



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
%matplotlib inline
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv(r"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv")

sns.countplot(data=df,x='gender')
sns.countplot(data=df,x='hsc_b')
sns.countplot(data=df,x='hsc_s')

sns.displot(data=df,x='ssc_b',col='gender',hue='gender')
sns.displot(data=df,x='hsc_b',col='gender',hue='gender')

sns.histplot(data=df,x='ssc_b',hue='status',kde=True)
sns.histplot(data=df,x='hsc_b',hue='status',kde=True)

sns.heatmap(df.drop(['sl_no','salary'],axis=1).corr(),cmap='Oranges',annot=True)

X1 = df.drop(columns=['sl_no','salary','status'])
y1 = df['status']
X1_train,X1_test,y1_train,y1_test = train_test_split(X1,y1,test_size=0.25)

random_forest = RandomForestClassifier(n_estimators=350, min_samples_leaf=2, random_state=42)
random_forest.fit(X_train, y_train)
y_pred = random_forest.predict(X_test)
accuracy_score(y_test,y_pred)

from sklearn.linear_model import LogisticRegression
clf=LogisticRegression()
clf.fit(X_train,y_train)




from sklearn import tree
model_2 = tree.DecisionTreeClassifier(criterion='entropy')
model_2.fit(X_train, y_train)
model_2.score(X_train,y_train)


